---
title: Value of Time Estimation
subtitle: Estimating the Value of Time (VOT) Parameter for Utah's Travel Demand Model
description: This notebook replicates the analysis from the '_archive/_Source - Med Income & Value of Time - 2022-08-09.xlsb' using the Utah Household and Hosehold Income data from 2019-2023 American Community Survey 5-Year Estimates. The VOTs are segmented by occupancy and purpose (work vs. personal).
author:
 - name: Pukar Bhandari
   email: pukar.bhandari@wfrc.utah.gov
   affiliation:
     - name: Wasatch Front Regional Council
       url: "https://wfrc.utah.gov/"
date: "2025-10-14"

execute:
   eval: true
jupyter: python3

format:
  html:
    theme:
        light: flatly
        dark: darkly
    respect-user-color-scheme: true

    fig-width: 8.4
    fig-height: 5.44
    fig-responsive: true

    toc: true
    number-sections: true
    number-depth: 2
    html-math-method: katex
    code-link: true
    code-tools: true
    code-fold: true
    code-summary: "Show the code"
    # fig-width: 8
    # fig-height: 5
    # out-width: "100%"
    # fig-align: center
    resources:
        - "_output/*.csv"

title-block-banner: true
---

## Introduction



## Environment Setup

### Install Required Packages

Install the necessary Python packages for data processing, spatial analysis, and visualization. This includes pandas for data manipulation, geopandas for spatial operations, pygris for accessing Census TIGER/Line shapefiles, and various visualization libraries.

``` python
!conda install -c conda-forge numpy pandas geopandas matplotlib seaborn python-dotenv openpyxl
!pip install pygris adjustText
```

### Load Libraries

Import all required libraries for the analysis. The pygris library enables direct access to Census Bureau geographic data and ACS estimates through their API.

```{python}
# For Analysis
import numpy as np
import pandas as pd
import geopandas as gpd
import warnings

# For Visualization
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import matplotlib.ticker as ticker
import seaborn as sns
from adjustText import adjust_text

# Census data query libraries & modules
from pygris import blocks, block_groups, counties, states
from pygris.helpers import validate_state, validate_county
from pygris.data import get_census

# misc
import datetime
import os
from pathlib import Path
import requests

from dotenv import load_dotenv
load_dotenv()
```

### Environment Variables

Set the coordinate reference system for the analysis to NAD83 / UTM zone 12N (EPSG:3566), which is appropriate for Utah. Also validate the state FIPS code for Utah.

```{python}
PROJECT_CRS = "EPSG:3566"  # NAD83 / UTM zone 12N
STATE_FIPS = validate_state("UT")
```

::: {.callout-tip}
**Need a Census API key?** Get one for free at [census.gov/developers](https://api.census.gov/data/key_signup.html).

Create a `.env` file in the project directory and add your Census API key:
```CENSUS_API_KEY=your-key-here```
This enables fetching US Census data from the Census API.
:::

```{python}
#| eval: false

# Set your API key into environment (alternative to .env file)
os.environ['CENSUS_API_KEY'] = 'your_api_key_here'
```

## Define Helper Functions

### Fetch Excel Files from BLS or BTS

```{python}
def fetch_excel(path, url):
    """
    Download Excel file if it doesn't exist locally.

    Parameters:
    -----------
    path : str or Path
        Local file path to save the Excel file
    url : str
        URL to download the Excel file from
    """
    # Convert to Path object if string
    filepath = Path(path)

    # Download file if it doesn't exist
    if not filepath.exists():
        filepath.parent.mkdir(parents=True, exist_ok=True)

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        response = requests.get(url, headers=headers)
        filepath.write_bytes(response.content)
```

## Lookup Tables

### Income Category Lookup

Create a reference table defining the income brackets used in ACS Table B19001. Each bracket has a lower and upper limit, and we calculate midpoints for median estimation. The highest bracket ($200,000+) uses $300,000 as a reasonable midpoint based on income distribution patterns.

```{python}
lookup_hhinc = pd.DataFrame({
  "Income Category": [
    "HH_LT_10K", "HH_10_15K", "HH_15_20K", "HH_20_25K", "HH_25_30K", "HH_30_35K",
    "HH_35_40K", "HH_40_45K", "HH_45_50K", "HH_50_60K", "HH_60_75K",
    "HH_75_100K", "HH_100_125K", "HH_125_150K", "HH_150_200K", "HH_GT_200K"
  ],
  "Lower Limit": [
    0, 10000, 15000, 20000, 25000, 30000,
    35000, 40000, 45000, 50000, 60000,
    75000, 100000, 125000, 150000, 200000
  ],
  "Upper Limit": [
    9999, 14999, 19999, 24999, 29999, 34999,
    39999, 44999, 49999, 59999, 74999,
    99999, 124999, 149999, 199999, np.inf
  ]
})

# Compute midpoint and round it
lookup_hhinc['Midpoint'] = (
  (lookup_hhinc['Lower Limit'] + lookup_hhinc['Upper Limit']) / 2
).round()

# Replace infinite midpoint (last category) with 300000
lookup_hhinc.loc[np.isinf(lookup_hhinc["Upper Limit"]), "Midpoint"] = 300000

lookup_hhinc
```

## Raw Data Sources

### Consumer Price Index

The Consumer Price Index (CPI-U-RS) from the Bureau of Labor Statistics provides inflation adjustment factors. This will be useful for comparing income values across different time periods or adjusting historical data to current dollars. The research series (R-CPI-U-RS) is particularly appropriate for longitudinal analysis as it uses consistent methodology throughout the time series.

Download and load the CPI data directly from the BLS website. The data includes annual average CPI values that can be used to adjust income figures for inflation.

Data Source: Consumer Price Index for All Urban Consumers (CPI-U) \[Source: [Bureau of Labor Statistics](https://www.bls.gov/cpi/research-series/r-cpi-u-rs-home.htm)\]

```{python}
# Usage
filepath_cpi = Path("_data/bls/r-cpi-u-rs-allitems.xlsx")
url_cpi = "https://www.bls.gov/cpi/research-series/r-cpi-u-rs-allitems.xlsx"

# Download if needed
fetch_excel(path=filepath_cpi, url=url_cpi)

# Read separately
df_CPI = pd.read_excel(
    filepath_cpi,
    sheet_name="All items",
    usecols="A:N",
    skiprows=5,
    engine='openpyxl'
)

df_CPI
```

### American Community Survey (ACS) 5-Year Estimates

#### Define Census Variables

Define the specific ACS variables we need for this analysis. Table B19013 provides median household income, while Table B19001 provides the household income distribution across 16 income brackets. These brackets allow us to calculate weighted averages and estimate medians when direct values are unavailable.

```{python}
# Define variables to download
acs_variables = {
    'B19013_001E': 'HH_MED_INC',  # Median Household Income in the Past 12 Months (in 2023 Inflation-Adjusted Dollars)
    'B19013_001M': 'HH_MED_INC_MOE',  # Margin of Error for Median Household Income
    'B19001_001E': 'HH_TOTAL',  # Total Households
    'B19001_002E': 'HH_LT_10K',  # Less than $10,000
    'B19001_003E': 'HH_10_15K',  # $10,000 to $14,999
    'B19001_004E': 'HH_15_20K',  # $15,000 to $19,999
    'B19001_005E': 'HH_20_25K',  # $20,000 to $24,999
    'B19001_006E': 'HH_25_30K',  # $25,000 to $29,999
    'B19001_007E': 'HH_30_35K',  # $30,000 to $34,999
    'B19001_008E': 'HH_35_40K',  # $35,000 to $39,999
    'B19001_009E': 'HH_40_45K',  # $40,000 to $44,999
    'B19001_010E': 'HH_45_50K',  # $45,000 to $49,999
    'B19001_011E': 'HH_50_60K',  # $50,000 to $59,999
    'B19001_012E': 'HH_60_75K',  # $60,000 to $74,999
    'B19001_013E': 'HH_75_100K',  # $75,000 to $99,999
    'B19001_014E': 'HH_100_125K',  # $100,000 to $124,999
    'B19001_015E': 'HH_125_150K',  # $125,000 to $149,999
    'B19001_016E': 'HH_150_200K',  # $150,000 to $199,999
    'B19001_017E': 'HH_GT_200K'  # $200,000 or more
}
```

#### State Level Data

Retrieve state-level income data for Utah. While we'll primarily use smaller geographies, state-level data provides a useful benchmark for comparison and validation.

```{python}
# Fetch state boundaries from TIGER/Line shapefiles
gdf_ut_bound = states(
  year=2023,
  cache=True
).to_crs(PROJECT_CRS)

# Filter for Utah only
gdf_ut_bound = gdf_ut_bound[gdf_ut_bound['STATEFP'] == str(STATE_FIPS)]

# Fetch Income data from ACS 5-year estimates for Utah
df_ut_income = get_census(
  dataset="acs/acs5",
  year=2023,
  variables=list(acs_variables.keys()),
  params={
      # "key": f"{os.getenv('CENSUS_API_KEY')}", # FIXME: This causes error
      "for": f"state:{STATE_FIPS}"
    },
    return_geoid=True,
    guess_dtypes=True
)

# Join ACS data to block group boundaries and transform CRS
gdf_ut_income = gdf_ut_bound[['GEOID', 'STATEFP', 'NAME', 'geometry']].merge(
    df_ut_income, on = "GEOID"
).to_crs(PROJECT_CRS).rename(columns=acs_variables)

# Preview data
gdf_ut_income
```

## Intermediate Calculations

### Income Groupings (Approximate Income Quartiles)

```{python}
# Create a copy of lookup table to work with
df_inc_group = lookup_hhinc.copy()

# Get the income category columns from gdf_ut_income
# Extract just the income bracket counts (excluding totals and medians)
income_cols = [col for col in gdf_ut_income.columns if col.startswith('HH_')
               and col not in ['HH_TOTAL', 'HH_MED_INC', 'HH_MED_INC_MOE']]

# Create a mapping between lookup categories and gdf_ut_income columns
# They should already match, but let's be explicit
df_inc_group['State HH'] = df_inc_group['Income Category'].map(
    gdf_ut_income[income_cols].iloc[0].to_dict()
)

# Calculate percentage of households
total_hh = df_inc_group['State HH'].sum()
df_inc_group['% HH'] = (df_inc_group['State HH'] / total_hh * 100).round(1)

# Calculate cumulative percentage
df_inc_group['Cum % HH'] = df_inc_group['% HH'].cumsum().round(1)

# Assign income groups based on quartiles (25%, 50%, 75%, 100%)
df_inc_group['Inc Group'] = pd.cut(
    df_inc_group['Cum % HH'],
    bins=[0, 25, 50, 75, 100],
    labels=['Inc Group 1', 'Inc Group 2', 'Inc Group 3', 'Inc Group 4'],
    include_lowest=True
)

# Calculate HH_MedInc_Product (HH * Midpoint)
df_inc_group['HH_MedInc_Product'] = df_inc_group['State HH'] * df_inc_group['Midpoint']

# Display the dataframe
df_inc_group
```


```{python}
#| fig-cap: "Household Income Distribution by Quartile"

# Prepare data for seaborn
df_plot = df_inc_group.copy()
df_plot['Income Label'] = df_plot['Income Category'].str.replace('HH_', '').str.replace('_', '\n')
df_plot['index'] = range(len(df_plot))

# Define colors for each income group
palette = {'Inc Group 1': '#3498db', 'Inc Group 2': '#2ecc71',
           'Inc Group 3': '#f39c12', 'Inc Group 4': '#e74c3c'}

# Set seaborn style and context
sns.set_style("whitegrid")
sns.set_context("notebook")

# Create barplot using seaborn
# plt.figure(figsize=(12, 6))
sns.barplot(
    data=df_plot,
    x='index',
    y='State HH',
    hue='Inc Group',
    palette=palette,
    legend=True,
    dodge=False
)

# Customize plot
plt.xlabel('Income Category', fontsize=11, fontweight='bold')
plt.ylabel('Number of Households', fontsize=11, fontweight='bold')
plt.title('Utah Household Income Distribution by Quartile',
          fontsize=13, fontweight='bold', pad=20)

# Format y-axis with comma separator
plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f'{x:,.0f}'))

# Set x-axis labels
plt.xticks(range(len(df_plot)), df_plot['Income Label'],
           rotation=45, ha='right', fontsize=9)

# Customize legend
plt.legend(loc='upper left', frameon=True, fontsize=10, title='')

# Grid styling
plt.grid(axis='y', alpha=0.3, linestyle='--')
plt.gca().set_axisbelow(True)

# Remove top and right spines for cleaner look
sns.despine()

plt.tight_layout()
plt.show()
```

### Median Income (in Model Base Year Dollars)

```{python}
# Define income categories
categories = {
    'Average': df_inc_group,
    'Low Inc': df_inc_group[df_inc_group['Inc Group'] == 'Inc Group 1'],
    'High Inc': df_inc_group[df_inc_group['Inc Group'] != 'Inc Group 1']
}

# Calculate metrics for each category
summary_data = {}
for cat_name, cat_df in categories.items():
    summary_data[cat_name] = {
        'Sum HH': cat_df['State HH'].sum(),
        'Sum HH * Inc': cat_df['HH_MedInc_Product'].sum(),
    }
    # Calculate unadjusted median income
    summary_data[cat_name]['Unadj Med Inc'] = (
        summary_data[cat_name]['Sum HH * Inc'] / summary_data[cat_name]['Sum HH']
    )

# Calculate correction factor from Average category
actual_median_income = gdf_ut_income['HH_MED_INC'].iloc[0]
correction_factor = actual_median_income / summary_data['Average']['Unadj Med Inc']
inflation_factor = 1.0

# Apply correction and inflation factors
for cat_name in summary_data:
    # summary_data[cat_name]['Correction Factor'] = correction_factor
    summary_data[cat_name]['Adj Med Income'] = (
        summary_data[cat_name]['Unadj Med Inc'] * correction_factor
    )
    # summary_data[cat_name]['Inflation Adj Factor'] = inflation_factor
    summary_data[cat_name]['Median Income'] = (
        summary_data[cat_name]['Adj Med Income'] * inflation_factor
    )

# Convert to DataFrame
df_summary = pd.DataFrame(summary_data).T

# Format for display
format_specs = {
    'Sum HH': '{:,.0f}',
    'Sum HH * Inc': '{:,.0f}',
    'Unadj Med Inc': '${:,.0f}',
    # 'Correction Factor': '{:.4f}',
    'Adj Med Income': '${:,.0f}',
    # 'Inflation Adj Factor': '{:.4f}',
    'Median Income': '${:,.0f}'
}

df_median_income = df_summary.copy()
for col, fmt in format_specs.items():
    df_median_income[col] = df_summary[col].apply(lambda x: fmt.format(x))

df_median_income
```

### Value of Time (in Model Base Year Dollars)

#### Calculate Hourly Income

```{python}
# Convert annual median income to hourly rate (assuming 2080 work hours/year)
df_hourly = pd.DataFrame({
    'Median Income': df_summary['Median Income'],
    'Hourly Rate': df_summary['Median Income'] / 2080
}, index=['Average', 'Low Inc', 'High Inc'])

df_hourly.style.format({
    'Median Income': '${:,.0f}',
    'Hourly Rate': '${:.2f}'
})
```

#### Setup VOT Calculation Helper

```{python}
# Define VOT as percentage of hourly income for each trip purpose and income group
# Hardcoded VOT percentages from previous calculations
vot_pct = pd.DataFrame({
    'Work': [0.39, 0.62, 0.34],
    'Personal': [0.30, 0.49, 0.27]
}, index=['Average', 'Low Inc', 'High Inc'])
```

```{python}
# Calculate VOT in cents per minute (hourly rate * percentage * 100 / 60)
vot_cents_min = ((df_hourly['Hourly Rate'].values[:, None] * vot_pct) * 100 / 60).round(0)

vot_cents_min.style.format({
    'Work': '${:.1f}',
    'Personal': '${:.1f}'
})
```

#### Calculate and Display Work & Personal VOT

```{python}
# Display Work VOT results
df_vot_work = pd.DataFrame({
    '% of Income': vot_pct['Work'],
    'Unrounded ($/hr)': df_hourly['Hourly Rate'] * vot_pct['Work'],
    'VOT (¢/min)': vot_cents_min['Work'],
    'Equivalent ($/hr)': vot_cents_min['Work'] * 60 / 100
}, index=['Average', 'Low Inc', 'High Inc'])

df_vot_work.style.format({
    '% of Income': '{:.0%}',
    'Unrounded ($/hr)': '${:.2f}',
    'VOT (¢/min)': '{:.0f}',
    'Equivalent ($/hr)': '${:.2f}'
})
```

```{python}
# Display Personal VOT results
df_vot_personal = pd.DataFrame({
    '% of Income': vot_pct['Personal'],
    'Unrounded ($/hr)': df_hourly['Hourly Rate'] * vot_pct['Personal'],
    'VOT (¢/min)': vot_cents_min['Personal'],
    'Equivalent ($/hr)': vot_cents_min['Personal'] * 60 / 100
}, index=['Average', 'Low Inc', 'High Inc'])

df_vot_personal.style.format({
    '% of Income': '{:.0%}',
    'Unrounded ($/hr)': '${:.2f}',
    'VOT (¢/min)': '{:.0f}',
    'Equivalent ($/hr)': '${:.2f}'
})
```

#### Calculate and Display Truck VOT

```{python}
# Calculate Truck VOT (using Average income as base)
# Hard coded percentages for truck types
truck_pct = pd.Series([0.65, 0.87, 1.10], index=['Light', 'Medium', 'Heavy'])

df_vot_trucks = pd.DataFrame({
    '% of Income': truck_pct,
    'Unrounded ($/hr)': df_hourly.loc['Average', 'Hourly Rate'] * truck_pct,
    'VOT (¢/min)': ((df_hourly.loc['Average', 'Hourly Rate'] * truck_pct) * 100 / 60).round(0)
})

df_vot_trucks['Equivalent ($/hr)'] = df_vot_trucks['VOT (¢/min)'] * 60 / 100

df_vot_trucks.style.format({
    '% of Income': '{:.0%}',
    'Unrounded ($/hr)': '${:.2f}',
    'VOT (¢/min)': '{:.0f}',
    'Equivalent ($/hr)': '${:.2f}'
})
```

## Export Results

### Create Final VOT Table

```{python}
# Build the final VOT parameters table
df_vot_params = pd.DataFrame({
    'Parameter': [
        'VOT_Auto_Wrk', 'VOT_Auto_Per', 'VOT_Auto_Ext',
        'VOT_LT', 'VOT_MD', 'VOT_HV',
        'VOT_Auto_Wrk_Lo', 'VOT_Auto_Per_Lo',
        'VOT_Auto_Wrk_Hi', 'VOT_Auto_Per_Hi'
    ],
    'VOT (cent/min)': [
        vot_cents_min.loc['Average', 'Work'],
        vot_cents_min.loc['Average', 'Personal'],
        (vot_cents_min.loc['Average', 'Work'] + vot_cents_min.loc['Average', 'Personal']) / 2,
        df_vot_trucks.loc['Light', 'VOT (¢/min)'],
        df_vot_trucks.loc['Medium', 'VOT (¢/min)'],
        df_vot_trucks.loc['Heavy', 'VOT (¢/min)'],
        vot_cents_min.loc['Low Inc', 'Work'],
        vot_cents_min.loc['Low Inc', 'Personal'],
        vot_cents_min.loc['High Inc', 'Work'],
        vot_cents_min.loc['High Inc', 'Personal']
    ]
})

df_vot_params['VOT ($/hr)'] = df_vot_params['VOT (cent/min)'] * 60 / 100

df_vot_params.style.format({
    'VOT (cent/min)': '{:.0f}¢',
    'VOT ($/hr)': '${:.1f}'
})
```

### Export to CSV

```{python}
# Create output directory if it doesn't exist
output_dir = Path("_output")
output_dir.mkdir(parents=True, exist_ok=True)

# Export to CSV
df_vot_params.to_csv(
    output_dir / "value_of_time.csv",
    index=False
)
```

::: {.callout-tip title="Download the output files:"}
[value_of_time.csv](./_output/value_of_time.csv)
:::
